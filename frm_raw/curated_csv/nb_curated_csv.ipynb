{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d63ee5fe-9d66-405b-adb1-dc89489b52c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ==========================================================\n",
    "# UNIFICAR ARCHIVOS CSV\n",
    "# Proyecto: Liga 1 Perú\n",
    "# Autor: Oscar García Del Águila\n",
    "# ==========================================================\n",
    "\n",
    "import bootstrap\n",
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "from util.utils_liga1 import setup_adls, get_dbutils, get_abfss_path, read_parquet_adls, write_parquet_adls\n",
    "from frm_raw.curated_csv.curated_csv import procesar_curated_csv\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# PARÁMETROS DESDE ADF\n",
    "# ----------------------------------------------------------\n",
    "dbutils.widgets.text(\"nombre_archivo\", \"\")\n",
    "dbutils.widgets.text(\"anio\", \"\")\n",
    "dbutils.widgets.text(\"filesystem\", \"\")\n",
    "dbutils.widgets.text(\"capa_raw\", \"\")\n",
    "dbutils.widgets.text(\"rutaBase\", \"\")\n",
    "dbutils.widgets.text(\"ModoEjecucion\", \"\")\n",
    "dbutils.widgets.text(\"FechaCarga\", \"\")\n",
    "\n",
    "nombre_archivo = dbutils.widgets.get(\"nombre_archivo\")\n",
    "anio = dbutils.widgets.get(\"anio\")\n",
    "filesystem = dbutils.widgets.get(\"filesystem\")\n",
    "capa_raw = dbutils.widgets.get(\"capa_raw\").strip(\"/\")\n",
    "rutaBase = dbutils.widgets.get(\"rutaBase\").strip(\"/\")\n",
    "ModoEjecucion = dbutils.widgets.get(\"ModoEjecucion\")\n",
    "FechaCarga = dbutils.widgets.get(\"FechaCarga\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CONFIGURACIÓN DE ADLS Y SPARK\n",
    "# ----------------------------------------------------------\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "setup_adls()\n",
    "dbutils = get_dbutils()\n",
    "\n",
    "print(\"===============================================\")\n",
    "print(\"PROCESO CURATED CSV → RAW\")\n",
    "print(\"===============================================\")\n",
    "print(f\"Entidad        : {nombre_archivo}\")\n",
    "print(f\"Año            : {anio}\")\n",
    "print(f\"Modo ejecución : {ModoEjecucion}\")\n",
    "print(\"===============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4550d188-c5be-4eb7-a1a5-3e71d2e9ce5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# RUTAS ORIGEN (STG) / DESTINO (RAW)\n",
    "# ----------------------------------------------------------\n",
    "if ModoEjecucion == \"HISTORICO\":\n",
    "    ruta_origen = f\"{capa_raw}/{rutaBase}/{nombre_archivo}/stg/{anio}/data\"\n",
    "    ruta_destino = f\"{capa_raw}/{rutaBase}/{nombre_archivo}/stg/{anio}/curated\"\n",
    "\n",
    "elif ModoEjecucion == \"REPROCESO\":\n",
    "    ruta_origen = f\"{capa_raw}/{rutaBase}/{nombre_archivo}/stg/{anio}/01/01/data\"\n",
    "    ruta_destino = f\"{capa_raw}/{rutaBase}/{nombre_archivo}/{anio}/01/01/data\"\n",
    "\n",
    "else:  # INCREMENTAL\n",
    "    fecha_fmt = FechaCarga.split(\" \")[0].replace(\"-\", \"/\")\n",
    "    ruta_origen = f\"{capa_raw}/{rutaBase}/{nombre_archivo}/stg/{fecha_fmt}/data\"\n",
    "    ruta_destino = f\"{capa_raw}/{rutaBase}/{nombre_archivo}/{fecha_fmt}/data\"\n",
    "\n",
    "\n",
    "ruta_abfss_origen = get_abfss_path(ruta_origen)\n",
    "ruta_abfss_destino = get_abfss_path(ruta_destino)\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# EJECUCIÓN SEGURA\n",
    "# ----------------------------------------------------------\n",
    "try:\n",
    "    print(\"===== LECTURA DESDE STG =====\")\n",
    "    df = read_parquet_adls(spark, ruta_abfss_origen)\n",
    "\n",
    "    print(\"===== APLICANDO CURATED CSV =====\")\n",
    "    df_proc = procesar_curated_csv(df)\n",
    "\n",
    "    print(\"===== ESCRITURA EN RAW =====\")\n",
    "    write_parquet_adls(df_proc, ruta_abfss_destino)\n",
    "    print(f\"Curated CSV completado correctamente.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error en Curated CSV\")\n",
    "    print(str(e))\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_curated_csv",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
