{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "475d2ed3-5a8a-4c00-92b2-4d0254870c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ==========================================================\n",
    "# UNIFICAR Y LIMPIAR PARQUETS HIST√ìRICOS (1FL)\n",
    "# Proyecto: Liga 1 Per√∫\n",
    "# Autor: Oscar Garc√≠a Del √Åguila\n",
    "# ==========================================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql import functions as F\n",
    "import traceback\n",
    "\n",
    "# Importar utilidades ADLS\n",
    "from util.utils_liga1 import setup_adls, get_abfss_path, get_dbutils\n",
    "\n",
    "# COMMAND ----------\n",
    "# üì¶ PARAMETROS DESDE ADF\n",
    "dbutils.widgets.text(\"filesystem\", \"\")\n",
    "dbutils.widgets.text(\"capa_raw\", \"\")\n",
    "dbutils.widgets.text(\"rutaBase\", \"\")\n",
    "dbutils.widgets.text(\"nombre_archivo\", \"\")\n",
    "dbutils.widgets.text(\"historical_start_year\", \"\")\n",
    "dbutils.widgets.text(\"current_year\", \"\")\n",
    "\n",
    "filesystem = dbutils.widgets.get(\"filesystem\")\n",
    "capa_raw = dbutils.widgets.get(\"capa_raw\").strip(\"/\")\n",
    "rutaBase = dbutils.widgets.get(\"rutaBase\").strip(\"/\")\n",
    "nombre_archivo = dbutils.widgets.get(\"nombre_archivo\")\n",
    "historical_start_year = int(dbutils.widgets.get(\"historical_start_year\"))\n",
    "current_year = int(dbutils.widgets.get(\"current_year\"))\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# COMMAND ----------\n",
    "# ‚öôÔ∏è Configurar conexi√≥n a ADLS\n",
    "setup_adls()\n",
    "base_path = get_abfss_path(f\"{capa_raw}/{rutaBase}/{nombre_archivo}\")\n",
    "dbutils = get_dbutils()\n",
    "\n",
    "print(\"===============================================\")\n",
    "print(\"  üß© UNIFICADOR HIST√ìRICO DE PARQUETS (1FL)\")\n",
    "print(\"===============================================\")\n",
    "print(f\"Entidad        : {nombre_archivo}\")\n",
    "print(f\"A√±os procesados: {historical_start_year} - {current_year}\")\n",
    "print(f\"Ruta base RAW  : {base_path}\")\n",
    "print(\"===============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a18f0d5-0362-4690-8ecf-7030f94a2212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# üîç Buscar carpetas temporales con datos Parquet\n",
    "rutas_temp = []\n",
    "for year in range(historical_start_year, current_year + 1):\n",
    "    ruta = f\"{base_path}/temp/{year}/data\"\n",
    "    try:\n",
    "        archivos = dbutils.fs.ls(ruta)\n",
    "        if any(f.name.endswith(\".parquet\") for f in archivos):\n",
    "            rutas_temp.append(ruta)\n",
    "    except Exception:\n",
    "        print(f\"‚ö†Ô∏è Carpeta no encontrada o vac√≠a: {ruta}\")\n",
    "\n",
    "if not rutas_temp:\n",
    "    raise Exception(\"‚ùå No se encontraron archivos temporales para unificar.\")\n",
    "\n",
    "print(f\"üìÇ Carpetas detectadas: {len(rutas_temp)}\")\n",
    "for r in rutas_temp:\n",
    "    print(f\" - {r}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# üßÆ Consolidar DataFrames con esquema flexible\n",
    "df_final = None\n",
    "columnas_ref = None\n",
    "\n",
    "for ruta in rutas_temp:\n",
    "    try:\n",
    "        df = spark.read.parquet(ruta)\n",
    "        print(f\"‚úÖ Le√≠do correctamente: {ruta}\")\n",
    "\n",
    "        # Inicializar columnas de referencia\n",
    "        if columnas_ref is None:\n",
    "            columnas_ref = df.columns\n",
    "        else:\n",
    "            faltantes = [c for c in columnas_ref if c not in df.columns]\n",
    "            nuevos = [c for c in df.columns if c not in columnas_ref]\n",
    "\n",
    "            for c in faltantes:\n",
    "                df = df.withColumn(c, F.lit(None))\n",
    "            for n in nuevos:\n",
    "                if n not in columnas_ref:\n",
    "                    columnas_ref.append(n)\n",
    "\n",
    "            df = df.select(columnas_ref)\n",
    "\n",
    "        # Append\n",
    "        df_final = df if df_final is None else df_final.unionByName(df, allowMissingColumns=True)\n",
    "\n",
    "    except AnalysisException as e:\n",
    "        print(f\"üí• Error leyendo {ruta}: {e.desc}\")\n",
    "    except Exception:\n",
    "        print(f\"üí• Error general en {ruta}: {traceback.format_exc()}\")\n",
    "\n",
    "if df_final is None:\n",
    "    raise Exception(\"‚ùå No se pudo generar DataFrame consolidado.\")\n",
    "\n",
    "total_filas = df_final.count()\n",
    "print(f\"‚úÖ Total filas unificadas: {total_filas}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# üíæ Guardar parquet consolidado final\n",
    "output_path = f\"{base_path}/1FL/data\"\n",
    "final_file = f\"{output_path}/{nombre_archivo}.parquet\"\n",
    "\n",
    "print(\"===============================================\")\n",
    "print(f\"üíæ Guardando consolidado final en: {final_file}\")\n",
    "print(\"===============================================\")\n",
    "\n",
    "(\n",
    "    df_final\n",
    "    .withColumn(\"fecha_unificacion\", F.current_timestamp())\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(final_file)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Consolidado guardado con √©xito ({total_filas} registros).\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# üßπ Eliminar carpetas temporales tras consolidar\n",
    "print(\"===============================================\")\n",
    "print(\"üßπ Iniciando limpieza de carpetas temporales...\")\n",
    "print(\"===============================================\")\n",
    "\n",
    "for ruta in rutas_temp:\n",
    "    try:\n",
    "        dbutils.fs.rm(ruta, True)\n",
    "        print(f\"üßº Eliminada: {ruta}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è No se pudo eliminar {ruta}: {e}\")\n",
    "\n",
    "print(\"‚úÖ Limpieza completada correctamente.\")\n",
    "print(\"üéØ Proceso de unificaci√≥n hist√≥rica finalizado con √©xito.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "unificar_1FL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
