{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "475d2ed3-5a8a-4c00-92b2-4d0254870c47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ==========================================================\n",
    "# UNIFICAR Y LIMPIAR PARQUETS HIST√ìRICOS (1FL)\n",
    "# Proyecto: Liga 1 Per√∫\n",
    "# Autor: Oscar Garc√≠a Del √Åguila\n",
    "# Descripci√≥n:\n",
    "#   1Ô∏è‚É£ Conecta a ADLS con secrets de Key Vault\n",
    "#   2Ô∏è‚É£ Lee los archivos temporales (temp/<a√±o>/data/)\n",
    "#   3Ô∏è‚É£ Valida esquemas y hace append con columnas faltantes\n",
    "#   4Ô∏è‚É£ Guarda el resultado final en 1FL/data/\n",
    "#   5Ô∏è‚É£ Elimina las carpetas temp/ tras consolidar\n",
    "# ==========================================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql import functions as F\n",
    "import traceback\n",
    "\n",
    "# Importar utilidades ADLS\n",
    "from util.utils_liga1 import setup_adls, get_abfss_path, test_conexion_adls\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# üì¶ PARAMETROS RECIBIDOS DESDE PIPELINE\n",
    "dbutils.widgets.text(\"filesystem\", \"\")\n",
    "dbutils.widgets.text(\"capa_raw\", \"\")\n",
    "dbutils.widgets.text(\"rutaBase\", \"\")\n",
    "dbutils.widgets.text(\"nombre_archivo\", \"\")\n",
    "dbutils.widgets.text(\"historical_start_year\", \"\")\n",
    "dbutils.widgets.text(\"current_year\", \"\")\n",
    "\n",
    "filesystem = dbutils.widgets.get(\"filesystem\")\n",
    "capa_raw = dbutils.widgets.get(\"capa_raw\").strip(\"/\")\n",
    "rutaBase = dbutils.widgets.get(\"rutaBase\").strip(\"/\")\n",
    "nombre_archivo = dbutils.widgets.get(\"nombre_archivo\")\n",
    "historical_start_year = int(dbutils.widgets.get(\"historical_start_year\"))\n",
    "current_year = int(dbutils.widgets.get(\"current_year\"))\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "print(\"===============================================\")\n",
    "print(\"  üß© UNIFICADOR HIST√ìRICO DE PARQUETS (1FL)\")\n",
    "print(\"===============================================\")\n",
    "print(f\"Entidad        : {nombre_archivo}\")\n",
    "print(f\"A√±os procesados: {historical_start_year} - {current_year}\")\n",
    "print(f\"Ruta RAW base  : {capa_raw}/{rutaBase}\")\n",
    "print(\"===============================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a18f0d5-0362-4690-8ecf-7030f94a2212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    setup_adls()\n",
    "    test_conexion_adls()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error al configurar conexi√≥n ADLS: {e}\")\n",
    "    dbutils.notebook.exit(\"FAILED\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# üß≠ FUNCIONES AUXILIARES\n",
    "def safe_list_dir(path):\n",
    "    try:\n",
    "        return [f.path for f in dbutils.fs.ls(path)]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "def align_schemas(df_main, df_new):\n",
    "    \"\"\"\n",
    "    Alinea columnas entre dos DataFrames para unionByName seguro.\n",
    "    \"\"\"\n",
    "    cols_main = set(df_main.columns)\n",
    "    cols_new = set(df_new.columns)\n",
    "    \n",
    "    # Columnas faltantes en df_new\n",
    "    for c in cols_main - cols_new:\n",
    "        df_new = df_new.withColumn(c, F.lit(None))\n",
    "    # Columnas nuevas en df_new\n",
    "    for c in cols_new - cols_main:\n",
    "        df_main = df_main.withColumn(c, F.lit(None))\n",
    "    \n",
    "    # Reordenar columnas para uni√≥n coherente\n",
    "    return df_main.select(sorted(df_main.columns)), df_new.select(sorted(df_main.columns))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# üß† 2Ô∏è‚É£ RECOLECTAR RUTAS TEMPORALES DISPONIBLES\n",
    "rutas_temp = []\n",
    "for anio in range(historical_start_year, current_year + 1):\n",
    "    ruta_temp = f\"{capa_raw}/{rutaBase}/{nombre_archivo}/temp/{anio}/data\"\n",
    "    rutas_temp.append(get_abfss_path(ruta_temp))\n",
    "\n",
    "rutas_existentes = [r for r in rutas_temp if len(safe_list_dir(r)) > 0]\n",
    "\n",
    "if not rutas_existentes:\n",
    "    raise Exception(f\"No se encontraron rutas temporales para {nombre_archivo}\")\n",
    "\n",
    "print(\"‚úÖ Rutas temporales encontradas:\")\n",
    "for r in rutas_existentes:\n",
    "    print(f\"  - {r}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# üß© 3Ô∏è‚É£ LEER Y UNIR TODOS LOS PARQUETS (CON VALIDACI√ìN DE ESQUEMAS)\n",
    "df_final = None\n",
    "\n",
    "for ruta in rutas_existentes:\n",
    "    try:\n",
    "        df_temp = spark.read.parquet(ruta)\n",
    "        print(f\"üìÑ Le√≠do {ruta}: {df_temp.count()} registros, {len(df_temp.columns)} columnas\")\n",
    "\n",
    "        if df_final is None:\n",
    "            df_final = df_temp\n",
    "        else:\n",
    "            df_final, df_temp = align_schemas(df_final, df_temp)\n",
    "            df_final = df_final.unionByName(df_temp, allowMissingColumns=True)\n",
    "\n",
    "    except AnalysisException:\n",
    "        print(f\"‚ö†Ô∏è No se pudo leer la ruta: {ruta}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error procesando {ruta}: {str(e)}\")\n",
    "\n",
    "if df_final is None:\n",
    "    raise Exception(\"‚ùå No se pudo leer ning√∫n archivo parquet temporal.\")\n",
    "\n",
    "print(f\"‚úÖ Total de registros unificados: {df_final.count()}\")\n",
    "print(f\"‚úÖ Columnas finales: {len(df_final.columns)} ‚Üí {df_final.columns}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# üíæ 4Ô∏è‚É£ GUARDAR ARCHIVO FINAL 1FL\n",
    "ruta_final = f\"{capa_raw}/{rutaBase}/{nombre_archivo}/1FL/data\"\n",
    "ruta_final_abfss = get_abfss_path(ruta_final)\n",
    "\n",
    "print(f\"üíæ Guardando archivo final en: {ruta_final_abfss}\")\n",
    "df_final.write.mode(\"overwrite\").parquet(ruta_final_abfss)\n",
    "print(\"‚úÖ Archivo hist√≥rico consolidado guardado correctamente.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# üßπ 5Ô∏è‚É£ ELIMINAR CARPETAS TEMPORALES\n",
    "print(\"üßπ Eliminando carpetas temporales...\")\n",
    "for anio in range(historical_start_year, current_year + 1):\n",
    "    ruta_temp_eliminar = get_abfss_path(f\"{capa_raw}/{rutaBase}/{nombre_archivo}/temp/{anio}\")\n",
    "    try:\n",
    "        dbutils.fs.rm(ruta_temp_eliminar, True)\n",
    "        print(f\"üóëÔ∏è Carpeta eliminada: {ruta_temp_eliminar}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è No se pudo eliminar {ruta_temp_eliminar}: {e}\")\n",
    "\n",
    "print(\"üéØ Proceso hist√≥rico completado con √©xito.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "unificar_y_limpiar_parquet_1FL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
