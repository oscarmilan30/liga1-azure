{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01268c7f-6d41-4402-a54c-392a9e5b2610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ==========================================================\n",
    "# EXECUTE JOB DDL - Proyecto Liga 1 Perú\n",
    "# Recibe nombre de archivo DDL y lo ejecuta\n",
    "# ==========================================================\n",
    "\n",
    "from env_setup import *\n",
    "from utils_liga1 import get_dbutils\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "dbutils = get_dbutils()\n",
    "\n",
    "dbutils dbutils.widgets.text(\"prm_name_ddl\", \"\")\n",
    "prm_name_ddl= dbutils.widgets.get(\"prm_name_ddl\") \n",
    "\n",
    "print(f\"Archivo DDL recibido: {prm_name_ddl}\")\n",
    "\n",
    "container_name = dbutils.secrets.get(scope=\"secretliga1\", key=\"filesystemname\")\n",
    "adls_account_name = dbutils.secrets.get(scope=\"secretliga1\", key=\"storageaccount\")\n",
    "catalog_name = \"adbliga1futbol\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05023e8b-f39c-4fca-950d-44402d8efaab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Celda 2: Función para ejecutar TABLA y VISTA en una sola ejecución\n",
    "def ejecutar_tabla_y_vista(nombre_base, container, storage_account, catalog):\n",
    "    \"\"\"\n",
    "    Ejecuta ambos: tabla y vista en una sola ejecución\n",
    "    nombre_base: \"ddl_md_catalogo_equipos\" (sin _view)\n",
    "    \"\"\"\n",
    "    archivos_a_ejecutar = []\n",
    "    \n",
    "    # 1. Determinar archivos a ejecutar\n",
    "    archivo_tabla = f\"{nombre_base}.sql\"\n",
    "    archivo_vista = f\"{nombre_base}_view.sql\"\n",
    "    \n",
    "    # Verificar si existen ambos archivos\n",
    "    carpeta_base = nombre_base  # misma carpeta para tabla y vista\n",
    "    ruta_tabla = get_workspace_path(f\"ddl_deploy/{carpeta_base}/{archivo_tabla}\")\n",
    "    ruta_vista = get_workspace_path(f\"ddl_deploy/{carpeta_base}/{archivo_vista}\")\n",
    "    \n",
    "    if os.path.exists(ruta_tabla):\n",
    "        archivos_a_ejecutar.append((\"TABLA\", ruta_tabla))\n",
    "        print(f\"Encontrado archivo de TABLA: {archivo_tabla}\")\n",
    "    else:\n",
    "        print(f\"No se encontró archivo de TABLA: {archivo_tabla}\")\n",
    "    \n",
    "    if os.path.exists(ruta_vista):\n",
    "        archivos_a_ejecutar.append((\"VISTA\", ruta_vista))\n",
    "        print(f\"Encontrado archivo de VISTA: {archivo_vista}\")\n",
    "    else:\n",
    "        print(f\"No se encontró archivo de VISTA: {archivo_vista}\")\n",
    "    \n",
    "    if not archivos_a_ejecutar:\n",
    "        raise FileNotFoundError(f\"No se encontraron archivos para: {nombre_base}\")\n",
    "    \n",
    "    # 2. Ejecutar en orden: primero tabla, luego vista\n",
    "    for tipo, ruta_archivo in archivos_a_ejecutar:\n",
    "        print(f\"\\n EJECUTANDO {tipo}: {os.path.basename(ruta_archivo)}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Leer archivo\n",
    "        with open(ruta_archivo, 'r', encoding='utf-8') as archivo:\n",
    "            contenido = archivo.read()\n",
    "        \n",
    "        # Reemplazar variables\n",
    "        contenido_procesado = contenido.replace(\"${container_name}\", container)\n",
    "        contenido_procesado = contenido_procesado.replace(\"${storage_account}\", storage_account)\n",
    "        contenido_procesado = contenido_procesado.replace(\"${catalog_name}\", catalog)\n",
    "        \n",
    "        # Ejecutar SQL\n",
    "        ejecutar_sql_dividido(contenido_procesado)\n",
    "        \n",
    "        print(f\"{tipo} ejecutada exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca1f6347-2218-48ff-b53d-63d8147dc0e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Celda 3: Función para ejecutar SQL dividido\n",
    "def ejecutar_sql_dividido(sql_content):\n",
    "    \"\"\"\n",
    "    Divide el SQL en statements y los ejecuta uno por uno\n",
    "    \"\"\"\n",
    "    # Dividir por punto y coma\n",
    "    statements = []\n",
    "    statement_actual = \"\"\n",
    "    \n",
    "    for linea in sql_content.split('\\n'):\n",
    "        linea_limpia = linea.strip()\n",
    "        \n",
    "        # Saltar líneas vacías o comentarios\n",
    "        if not linea_limpia or linea_limpia.startswith('--'):\n",
    "            continue\n",
    "            \n",
    "        statement_actual += linea + \"\\n\"\n",
    "        \n",
    "        # Si termina con punto y coma, es un statement completo\n",
    "        if linea.rstrip().endswith(';'):\n",
    "            statements.append(statement_actual.strip())\n",
    "            statement_actual = \"\"\n",
    "    \n",
    "    # Si queda algo sin punto y coma, agregarlo\n",
    "    if statement_actual.strip():\n",
    "        statements.append(statement_actual.strip())\n",
    "    \n",
    "    print(f\"Encontrados {len(statements)} statements SQL\")\n",
    "    \n",
    "    # Ejecutar cada statement\n",
    "    for i, statement in enumerate(statements, 1):\n",
    "        print(f\"\\nEjecutando statement {i}/{len(statements)}:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Mostrar primeras líneas del statement\n",
    "        lineas = statement.split('\\n')\n",
    "        for j, linea in enumerate(lineas[:3]):  # Mostrar primeras 3 líneas\n",
    "            print(f\"   {linea}\")\n",
    "        if len(lineas) > 3:\n",
    "            print(\"   ...\")\n",
    "            \n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        try:\n",
    "            spark.sql(statement)\n",
    "            print(f\"Statement {i} ejecutado exitosamente\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error en statement {i}: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad8a90b7-36d8-410d-ba47-620d02e186cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "# Celda 4: Ejecución principal CORREGIDA\n",
    "try:\n",
    "    print(\"\\n INICIANDO EJECUCIÓN DDL AUTOMÁTICA...\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\" Parámetro recibido: {prm_name_ddl}\")\n",
    "    \n",
    "    # Validar parámetro obligatorio\n",
    "    if not prm_name_ddl or not prm_name_ddl.strip():\n",
    "        raise ValueError(\"El parámetro 'prm_name_ddl' está VACÍO\")\n",
    "    \n",
    "    # Ejecutar TABLA y VISTA automáticamente\n",
    "    ejecutar_tabla_y_vista(\n",
    "        prm_name_ddl,  # \"ddl_md_catalogo_equipos\"\n",
    "        container_name, \n",
    "        adls_account_name, \n",
    "        catalog_name\n",
    "    )\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"DDL COMPLETADO: Tabla y Vista creadas para {prm_name_ddl}\")\n",
    "    \n",
    "    # SOLO para éxito - usar notebook.exit()\n",
    "    dbutils.notebook.exit(\"SUCCESS\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"ERROR CRÍTICO: {str(e)}\")\n",
    "    \n",
    "    # PARA ERRORES - lanzar excepción (NO usar notebook.exit())\n",
    "    # Esto hará que el job marque como FAILED en Databricks\n",
    "    raise Exception(f\"Fallo en ejecución DDL: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "execute_job_ddl",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
