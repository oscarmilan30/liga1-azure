{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01268c7f-6d41-4402-a54c-392a9e5b2610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ==========================================================\n",
    "# EXECUTE JOB DDL - Proyecto Liga 1 Perú\n",
    "# Recibe nombre de archivo DDL y lo ejecuta\n",
    "# ==========================================================\n",
    "\n",
    "from env_setup import *\n",
    "from utils_liga1 import get_dbutils\n",
    "import os\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "dbutils = get_dbutils()\n",
    "\n",
    "dbutils.widgets.text(\"prm_name_ddl\", \"\")\n",
    "prm_name_ddl= dbutils.widgets.get(\"prm_name_ddl\") \n",
    "\n",
    "print(f\"Archivo DDL recibido: {prm_name_ddl}\")\n",
    "\n",
    "container_name = dbutils.secrets.get(scope=\"secretliga1\", key=\"filesystemname\")\n",
    "adls_account_name = dbutils.secrets.get(scope=\"secretliga1\", key=\"storageaccount\")\n",
    "catalog_name = \"adbliga1futbol\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05023e8b-f39c-4fca-950d-44402d8efaab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def execute_ddl_file(file_path):\n",
    "    \"\"\"Ejecuta un archivo DDL reemplazando las variables\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            ddl_content = file.read()\n",
    "        \n",
    "        # Reemplazar variables\n",
    "        ddl_content = ddl_content.replace(\"${catalog_name}\", catalog_name)\n",
    "        ddl_content = ddl_content.replace(\"${container_name}\", container_name)\n",
    "        ddl_content = ddl_content.replace(\"${storage_account}\", adls_account_name)\n",
    "                \n",
    "        spark.sql(ddl_content)\n",
    "        print(f\"DDL ejecutado exitosamente: {os.path.basename(file_path)}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error ejecutando DDL {file_path}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def process_ddl_files(ddl_name):\n",
    "    \"\"\"Procesa todos los archivos DDL para un objeto específico\"\"\"\n",
    "    base_path = get_workspace_path(f\"ddl_deploy/{ddl_name}\")\n",
    "    \n",
    "    if not os.path.exists(base_path):\n",
    "        print(f\"No se encuentra: ddl_deploy/{ddl_name}\")\n",
    "        return False\n",
    "    \n",
    "    # Buscar archivos SQL\n",
    "    sql_files = [f for f in os.listdir(base_path) if f.endswith('.sql')]\n",
    "    \n",
    "    if not sql_files:\n",
    "        print(f\"No hay archivos SQL en: {ddl_name}\")\n",
    "        return False\n",
    "    \n",
    "    # Identificar archivos\n",
    "    table_file = f\"{ddl_name}.sql\"\n",
    "    view_file = f\"{ddl_name}_view.sql\"\n",
    "    \n",
    "    executed_count = 0\n",
    "    \n",
    "    # Ejecutar tabla primero\n",
    "    if table_file in sql_files:\n",
    "        file_path = os.path.join(base_path, table_file)\n",
    "        if execute_ddl_file(file_path):\n",
    "            executed_count += 1\n",
    "    \n",
    "    # Ejecutar vista después\n",
    "    if view_file in sql_files:\n",
    "        file_path = os.path.join(base_path, view_file)\n",
    "        if execute_ddl_file(file_path):\n",
    "            executed_count += 1\n",
    "    \n",
    "    # Archivos adicionales\n",
    "    other_files = [f for f in sql_files if f not in [table_file, view_file]]\n",
    "    for sql_file in other_files:\n",
    "        file_path = os.path.join(base_path, sql_file)\n",
    "        if execute_ddl_file(file_path):\n",
    "            executed_count += 1\n",
    "    \n",
    "    print(f\"Total ejecutados: {executed_count} archivos\")\n",
    "    return executed_count > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ad8a90b7-36d8-410d-ba47-620d02e186cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ejecutar el proceso principal\n",
    "try:\n",
    "    print(\"Iniciando ejecución DDL...\")\n",
    "    success = process_ddl_files(prm_name_ddl)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"DDL completado: {prm_name_ddl}\")\n",
    "    else:\n",
    "        print(f\"Falló DDL: {prm_name_ddl}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error crítico: {str(e)}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "execute_job_ddl",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
