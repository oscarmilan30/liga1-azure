{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96525f57-1526-4e17-bf9c-83a8b5614984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ==========================================================\n",
    "# UDV - ESTADIOS\n",
    "# Proyecto: Liga 1 Perú\n",
    "# Autor: Oscar García Del Águila\n",
    "# ==========================================================\n",
    "\n",
    "from env_setup import *\n",
    "from utils_liga1 import setup_adls, get_dbutils, generar_udv_json, get_entity_data, get_abfss_path, is_dataframe_empty, get_predecesor,get_pipeline_params, get_yaml_from_param, write_delta_udv, log, read_udv_table, get_pipeline, extract_entity_name\n",
    "from pyspark.sql import SparkSession\n",
    "from md_estadios import carga_final\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CONFIGURACIÓN INICIAL\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "log(\"Inicio de ejecución del pipeline UDV\", \"INFO\", \"md_estadios\")\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "setup_adls()\n",
    "dbutils = get_dbutils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5aa2dc0-865e-4670-945c-7926382d882c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# PARÁMETROS Y PREDECESORES\n",
    "# ----------------------------------------------------------\n",
    "try:\n",
    "    # Para pruebas puedes dejar un valor por defecto; luego ADF lo setea\n",
    "    dbutils.widgets.text(\"prm_pipelineid\", \"7\")\n",
    "    prm_pipelineid = int(dbutils.widgets.get(\"prm_pipelineid\"))\n",
    "\n",
    "    pipeline_name = get_pipeline(prm_pipelineid)\n",
    "    entity_name = pipeline_name[\"pipeline\"]               # ej: 'md_estadios'\n",
    "\n",
    "    dict_predecesores = get_predecesor(prm_pipelineid)\n",
    "    dict_params       = get_pipeline_params(prm_pipelineid)\n",
    "\n",
    "    prm_ruta_tabla_pred = dict_predecesores[\"RutaTabla\"]  # ej: 'tb_udv.md_catalogo_equipos'\n",
    "\n",
    "    prm_capa_udv     = dict_params[\"CAPA_UDV\"]\n",
    "    prm_ruta_base    = dict_params[\"RUTA_BASE\"]\n",
    "    prm_ruta_tabla   = dict_params[\"RUTA_TABLA\"]\n",
    "    prm_formato      = dict_params[\"FORMATO_SALIDA\"]\n",
    "    prm_schema_tb    = dict_params[\"SCHEMA_TABLA\"]\n",
    "    prm_tabla_output = dict_params[\"NOMBRE_TABLA\"]\n",
    "    prm_ruta_yaml    = dict_params[\"YAML_PATH\"]\n",
    "\n",
    "    prm_ruta_tabla_output = f\"{prm_capa_udv}/{prm_ruta_base}/{prm_ruta_tabla}\"\n",
    "    ruta_delta_udv = get_abfss_path(prm_ruta_tabla_output)\n",
    "\n",
    "    log(\"Parámetros cargados correctamente\", \"INFO\", entity_name)\n",
    "\n",
    "except Exception as e:\n",
    "    log(f\"Error al cargar parámetros o predecesores: {e}\", \"ERROR\", \"md_estadios\")\n",
    "    print(traceback.format_exc())\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "970285b2-32fd-46a0-ae53-b0de937f11e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# LECTURA YAML\n",
    "# ----------------------------------------------------------\n",
    "try:\n",
    "    yaml_conf = get_yaml_from_param(prm_ruta_yaml)\n",
    "\n",
    "    prm_cols_catalogo_equipos    = yaml_conf[entity_name][\"cols_catalogo_equipos\"]\n",
    "    prm_cols_estadios            = yaml_conf[entity_name][\"cols_estadios\"]\n",
    "    prm_drop_duplicates_estadios = yaml_conf[entity_name][\"drop_duplicates_estadios\"]\n",
    "    prm_rename_columns           = yaml_conf[entity_name][\"rename_columns\"]\n",
    "    prm_schema                   = yaml_conf[entity_name][\"schema\"]\n",
    "    prm_dedup_cols               = yaml_conf[entity_name][\"dedup_cols\"]\n",
    "\n",
    "    log(\"YAML cargado correctamente\", \"INFO\", entity_name)\n",
    "\n",
    "except Exception as e:\n",
    "    log(f\"Error al leer YAML {prm_ruta_yaml}: {e}\", \"ERROR\", entity_name)\n",
    "    print(traceback.format_exc())\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0151840-84a9-42e4-ab9a-9ad2991a5fa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# LECTURA PREDECESORES (CATÁLOGO + RAW)\n",
    "# ----------------------------------------------------------\n",
    "try:\n",
    "    log(\"Lectura desde UDV/RAW\", \"INFO\", entity_name)\n",
    "\n",
    "    # 1) Catálogo de equipos (md_catalogo_equipos) vía RutaTabla\n",
    "    df_catalogo_equipos = read_udv_table(prm_ruta_tabla_pred)\n",
    "\n",
    "    if is_dataframe_empty(df_catalogo_equipos):\n",
    "        raise Exception(f\"No se encontró data en la tabla predecesora: {prm_ruta_tabla_pred}\")\n",
    "\n",
    "    # 2) RAW estadios desde tbl_paths (flg_udv = 'N')\n",
    "    entity_raw = extract_entity_name(entity_name)  # 'md_estadios' -> 'estadios'\n",
    "\n",
    "    df_raw_estadios = get_entity_data(entity_raw,dedup_cols=prm_dedup_cols)\n",
    "\n",
    "    if is_dataframe_empty(df_raw_estadios):\n",
    "        raise Exception(f\"No se encontró data en RAW para la entidad: {entity_raw}\")\n",
    "\n",
    "    log(\"Predecesores completados correctamente\", \"INFO\", entity_name)\n",
    "\n",
    "except Exception as e:\n",
    "    log(f\"Error en lectura de predecesores/RAW: {e}\", \"ERROR\", entity_name)\n",
    "    print(traceback.format_exc())\n",
    "    raise\n",
    "\n",
    "log(\"Finalización de la etapa de lectura\", \"INFO\", entity_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3a0aaeb-eb5b-4cfe-b10b-b24bed856b42",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763360475731}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp, date_format, lower,trim\n",
    "from utils_liga1 import cast_dataframe_schema, rename_columns\n",
    "\n",
    "# 1) Selección de catálogo\n",
    "df_catalogo_equipos_select = df_catalogo_equipos.select(*prm_cols_catalogo_equipos)\n",
    "display(df_catalogo_equipos_select)\n",
    "\n",
    "# 2) Selección y deduplicación de RAW estadios\n",
    "df_raw_estadios_select = (\n",
    "    df_raw_estadios\n",
    "    .select(*prm_cols_estadios)\n",
    "    .dropDuplicates(prm_drop_duplicates_estadios)\n",
    "    .select(\n",
    "        *[col(c) for c in prm_cols_estadios],\n",
    "        lower(col(\"club\") ).alias(\"club_lower\")\n",
    "    )\n",
    ")\n",
    "\n",
    "display(df_raw_estadios_select)\n",
    "# 3) JOIN catálogo vs estadios\n",
    "df_join = (\n",
    "    df_catalogo_equipos_select.alias(\"a\")\n",
    "    .join(\n",
    "        df_raw_estadios_select.alias(\"b\"),\n",
    "        trim(lower(col(\"a.nombre_transfermarkt\"))) == trim(col(\"b.club_lower\")),\n",
    "        \"left\"\n",
    "    )\n",
    "    .select(\n",
    "        col(\"a.id_equipo\"),\n",
    "        col(\"a.nombre_equipo\"),\n",
    "        col(\"b.club_lower\"),\n",
    "        col(\"b.estadio\"),\n",
    "        col(\"b.capacidad\"),\n",
    "        col(\"b.aforo\"),\n",
    "        col(\"b.fuente\"),\n",
    "        current_timestamp().alias(\"fecha_carga\"),\n",
    "        date_format(current_timestamp(), \"yyyyMMdd\").alias(\"periododia\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4) Renombrar columnas según YAML (club_lower → club_raw, fuente → fuente_estadio)\n",
    "df_rename = rename_columns(df_join, prm_rename_columns)\n",
    "\n",
    "# 5) Castear y ordenar columnas según schema\n",
    "df_cast = cast_dataframe_schema(df_rename, prm_schema)\n",
    "\n",
    "display(df_cast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3341c736-36cc-4700-8098-a9b597e9ac02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # ----------------------------------------------------------\n",
    "# # EJECUCIÓN PRINCIPAL\n",
    "# # ----------------------------------------------------------\n",
    "# try:\n",
    "#     log(\"Inicio de Ejecución Principal\", \"INFO\", entity_name)\n",
    "\n",
    "#     df_final = carga_final(\n",
    "#         df_catalogo_equipos,\n",
    "#         df_raw_estadios,\n",
    "#         prm_cols_catalogo_equipos,\n",
    "#         prm_cols_estadios,\n",
    "#         prm_drop_duplicates_estadios,\n",
    "#         prm_rename_columns,\n",
    "#         prm_schema\n",
    "#     )\n",
    "\n",
    "#     # MERGE simple (UPSERT) sobre id_equipo\n",
    "#     write_delta_udv(\n",
    "#         spark,\n",
    "#         df_final,\n",
    "#         prm_schema_tb,\n",
    "#         prm_tabla_output,\n",
    "#         mode=\"merge\",\n",
    "#         merge_condition=\"delta.id_equipo = df.id_equipo\"\n",
    "#     )\n",
    "\n",
    "#     log(\"Proceso completado correctamente\", \"SUCCESS\", entity_name)\n",
    "\n",
    "# except Exception as e:\n",
    "#     log(f\"Error en ejecución principal: {e}\", \"ERROR\", entity_name)\n",
    "#     print(traceback.format_exc())\n",
    "#     raise\n",
    "\n",
    "# log(\"Finalización del pipeline UDV\", \"INFO\", entity_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_md_estadios",
   "widgets": {
    "prm_pipelineid": {
     "currentValue": "7",
     "nuid": "81f2dc1c-1add-4611-9abc-3f5e3d5d67c6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "7",
      "label": null,
      "name": "prm_pipelineid",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "7",
      "label": null,
      "name": "prm_pipelineid",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
