{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4be45e1-bed0-4cb2-a372-c8163851c7ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ==========================================================\n",
    "# UDV - CATALOGO_EQUIPOS\n",
    "# Proyecto: Liga 1 Perú\n",
    "# Autor: Oscar García Del Águila\n",
    "# ==========================================================\n",
    "\n",
    "from env_setup import *\n",
    "from pyspark.sql import SparkSession\n",
    "from utils_liga1 import setup_adls, get_dbutils, get_abfss_path, read_parquet_adls, is_dataframe_empty,get_predecesor, get_pipeline_params,get_yaml_from_param,write_delta_udv, log\n",
    "from m_catalogo_equipos import select_final\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# CONFIGURACIÓN INICIAL\n",
    "# ----------------------------------------------------------\n",
    "entity_name = \"m_catalogo_equipos\"\n",
    "log(\"Inicio de ejecución del pipeline UDV\", \"INFO\", entity_name)\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "setup_adls()\n",
    "dbutils = get_dbutils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77b641e0-d341-427a-b7e6-47877c0dbb53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# PARÁMETROS Y PREDECESORES\n",
    "# ----------------------------------------------------------\n",
    "try:\n",
    "    dbutils.widgets.text(\"prm_pipelineid\", \"\")\n",
    "    prm_pipelineid = dbutils.widgets.get(\"prm_pipelineid\")\n",
    "\n",
    "    dict_predecesores = get_predecesor(prm_pipelineid)\n",
    "    dict_params = get_pipeline_params(prm_pipelineid)\n",
    "\n",
    "    prm_ruta_predecesor = dict_predecesores[\"Ruta_Predecesor\"]\n",
    "    ruta_abfss_origen = get_abfss_path(prm_ruta_predecesor)\n",
    "\n",
    "    prm_filesystem = dict_params[\"FILESYSTEM\"]\n",
    "    prm_capa_udv = dict_params[\"CAPA_UDV\"]\n",
    "    prm_ruta_base = dict_params[\"RUTA_BASE\"]\n",
    "    prm_ruta_tabla = dict_params[\"RUTA_TABLA\"]\n",
    "    prm_formato = dict_params[\"FORMATO_SALIDA\"]\n",
    "    prm_schema_tb = dict_params[\"SCHEMA_TABLA\"]\n",
    "    prm_tabla_output = dict_params[\"NOMBRE_TABLA\"]\n",
    "    prm_ruta_yaml = dict_params[\"YAML_PATH\"]\n",
    "\n",
    "    prm_ruta_tabla_output = f\"{prm_capa_udv}/{prm_ruta_base}/{prm_ruta_tabla}\"\n",
    "    ruta_delta_udv = get_abfss_path(prm_ruta_tabla_output)\n",
    "\n",
    "    log(\"Parámetros cargados correctamente\", \"INFO\", entity_name)\n",
    "\n",
    "except Exception as e:\n",
    "    log(f\"Error al cargar parámetros o predecesores: {e}\", \"ERROR\", entity_name)\n",
    "    dbutils.notebook.exit(f\"[FAILED] Error inicial en {entity_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a304f9c-71cb-480e-8892-d58d5fc5a207",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# LECTURA YAML\n",
    "# ----------------------------------------------------------\n",
    "try:\n",
    "    yaml_conf = get_yaml_from_param(prm_ruta_yaml)\n",
    "    prm_cols = yaml_conf[entity_name][\"cols\"]\n",
    "    prm_schema = yaml_conf[entity_name][\"schema\"]\n",
    "    prm_columns_sql = yaml_conf[entity_name][\"columns_sql\"]\n",
    "    prm_table_comment = yaml_conf[entity_name][\"table_comment\"]\n",
    "    log(\"YAML cargado correctamente\", \"INFO\", entity_name)\n",
    "except Exception as e:\n",
    "    log(f\"Error al leer YAML {prm_ruta_yaml}: {e}\", \"ERROR\", entity_name)\n",
    "    dbutils.notebook.exit(f\"[FAILED] Error YAML en {entity_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aca7bb14-ff33-4ed8-bced-e7ac04fbc746",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------\n",
    "# EJECUCIÓN PRINCIPAL\n",
    "# ----------------------------------------------------------\n",
    "try:\n",
    "    log(\"Lectura desde RAW/DATA\", \"INFO\", entity_name)\n",
    "    df = read_parquet_adls(spark, ruta_abfss_origen)\n",
    "\n",
    "    if is_dataframe_empty(df):\n",
    "        raise Exception(f\"No se encontró data en la ruta origen: {ruta_abfss_origen}\")\n",
    "\n",
    "    log(\"Procesamiento en UDV\", \"INFO\", entity_name)\n",
    "    df_final = select_final(df, prm_cols, prm_schema)\n",
    "\n",
    "    log(\"Escritura en capa UDV\", \"INFO\", entity_name)\n",
    "    write_delta_udv(\n",
    "        spark,\n",
    "        df_final,\n",
    "        schema=prm_schema_tb,\n",
    "        table_name=prm_tabla_output,\n",
    "        abfss_path=ruta_delta_udv,\n",
    "        formato=prm_formato,\n",
    "        mode=\"overwrite\",\n",
    "        columns_sql=prm_columns_sql,\n",
    "        table_comment=prm_table_comment\n",
    "    )\n",
    "\n",
    "    log(\"Proceso completado correctamente\", \"SUCCESS\", entity_name)\n",
    "    dbutils.notebook.exit(f\"[OK] Ejecución satisfactoria - {entity_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    log(f\"Error en ejecución: {e}\", \"ERROR\", entity_name)\n",
    "    print(traceback.format_exc())\n",
    "    dbutils.notebook.exit(f\"[FAILED] Error en {entity_name}: {str(e)}\")\n",
    "\n",
    "log(\"Finalización del pipeline UDV\", \"INFO\", entity_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "nb_m_catalogo_equipos",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
